Comparison:
time ./sRead 10:             time ./aRead 10:           time ./tRead 10:
real	0.113s               real: 0.015s               real: 0.016s

time ./sRead 100:            time ./aRead 100:          time ./tRead 100:
real	1.088s               real: 0.016s               real: 0.023s

time ./sRead 1000:           time ./aRead 1000:         time ./tRead 1000:
real	10.956s              real: 0.020s               real: 0.086s

time ./sRead 10000:          time ./aRead 10000:        time ./tRead 10000:
real :  1m48.771s            real: 0.041s               real: 0.789s

The time results between tRead and aRead using small number of blocks are 
pretty identical, both are faster than sRead. But when it comes to large 
number of blocks, after several tests using large number of blocks, tRead 
starts getting slower than aRead but still way faster than sRead. The reason
behind this I think, may due to the overhead it created while creating new threads 
for each disk read. 
